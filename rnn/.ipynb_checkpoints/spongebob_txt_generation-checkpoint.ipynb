{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PyTorch to Generate Spongebob Transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. import libraries we will depend on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Set variables for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = Namespace(\n",
    "    train_file='../web_scraper/spongebob-transcript.txt',\n",
    "    seq_size=32,\n",
    "    batch_size=16,\n",
    "    embedding_size=64,\n",
    "    lstm_size=64,\n",
    "    gradients_norm=5,\n",
    "    initial_words=['_________________________________________________________'],\n",
    "    predict_top_k=5,\n",
    "    checkpoint_path='checkpoint'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define a function to process the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_from_file(train_file, batch_size, seq_size):\n",
    "    with open(train_file, 'r') as file:\n",
    "        text = file.read()\n",
    "    text = text.split()\n",
    "    \n",
    "    word_counts = Counter(text)\n",
    "    \n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
    "    n_vocab = len(int_to_vocab)\n",
    "    \n",
    "    int_text = [vocab_to_int[w] for w in text]\n",
    "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
    "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
    "    out_text = np.zeros_like(in_text)\n",
    "    out_text[:-1] = in_text[1:]\n",
    "    out_text[-1] = in_text[0]\n",
    "    in_text = np.reshape(in_text, (batch_size, -1))\n",
    "    out_text = np.reshape(out_text, (batch_size, -1))\n",
    "    \n",
    "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text\n",
    "\n",
    "#Call the function and set some variables\n",
    "device = torch.device('cpu')\n",
    "int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = process_data_from_file(flags.train_file, flags.batch_size, flags.seq_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Take a look at some of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:\n",
      " 71566\n",
      "Out Text Matrix:\n",
      " [[12942   265 23161 ...   543   227    72]\n",
      " [  647    82   196 ...     1    89    30]\n",
      " [    8     3  4628 ...   188    70 15392]\n",
      " ...\n",
      " [ 2404    22    76 ... 11386    29  8081]\n",
      " [ 1060   131     0 ...  4740     5 14194]\n",
      " [ 7903  4002   203 ...   182   626  2083]]\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size:\\n', n_vocab)\n",
    "print('Out Text Matrix:\\n', out_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Now let's:\n",
    "5. Create the network in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModule(nn.Module):\n",
    "    #define necessary layers in the constructor:\n",
    "    #embedding layer, LSTM layer, dense layer\n",
    "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
    "        super(RNNModule, self).__init__()\n",
    "        self.seq_size = seq_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, lstm_size, batch_first=True)\n",
    "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
    "    \n",
    "    #define function for forward pass\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.dense(output)\n",
    "        return logits, state\n",
    "    \n",
    "    #define function to reset state at each epoch\n",
    "    def zero_state(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.lstm_size),\n",
    "                torch.zeros(1, batch_size, self.lstm_size))\n",
    "\n",
    "#Instantiate the network\n",
    "net = RNNModule(n_vocab, flags.seq_size, flags.embedding_size, flags.lstm_size)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Define a function to handle loss and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_and_train_op(net, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    return criterion, optimizer\n",
    "\n",
    "#Call the function\n",
    "criterion, optimizer = get_loss_and_train_op(net, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Define the function to get batches for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(in_text, out_text, batch_size, seq_size):\n",
    "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
    "    for i in range(0, num_batches * seq_size, seq_size):\n",
    "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Define the prediction function which will be used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
    "    # tells the network we are about to evaluate\n",
    "    net.eval()\n",
    "    \n",
    "    state_h, state_c = net.zero_state(1)\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    for w in words:\n",
    "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "    \n",
    "    _, top_ix = torch.topk(output[0], k=top_k)\n",
    "    choices = top_ix.tolist()\n",
    "    choice = np.random.choice(choices[0])\n",
    "    \n",
    "    # append another word\n",
    "    words.append(int_to_vocab[choice])\n",
    "    \n",
    "    # append 100 more\n",
    "    for _ in range(100):\n",
    "        ix = torch.tensor([[choice]]).to(device)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "        \n",
    "        _, top_ix = torch.topk(output[0], k=top_k)\n",
    "        choices = top_ix.tolist()\n",
    "        choice = np.random.choice(choices[0])\n",
    "        words.append(int_to_vocab[choice])\n",
    "    \n",
    "    print(' '.join(words))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Training! Loop through batches for each epoch, compute losses and update network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iteration = 0\n",
    "for e in range(50):\n",
    "    print('Epoch: ', e)\n",
    "    batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)\n",
    "    state_h, state_c = net.zero_state(flags.batch_size)\n",
    "    \n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    for x, y in batches:\n",
    "        iteration += 1\n",
    "        print('Iteration: ', iteration)\n",
    "        \n",
    "        # train\n",
    "        net.train()\n",
    "        \n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x = torch.tensor(x).to(device)\n",
    "        y = torch.tensor(y).to(device)\n",
    "            \n",
    "        logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
    "        loss = criterion(logits.transpose(1,2), y)\n",
    "        \n",
    "        # detach() so pytorch can calculate loss\n",
    "        state_h = state_h.detach()\n",
    "        state_c = state_c.detach()\n",
    "        \n",
    "        loss_value = loss.item()\n",
    "        \n",
    "        # back propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient clipping \n",
    "        _ = torch.nn.utils.clip_grad_norm_(net.parameters(), flags.gradients_norm)\n",
    "        \n",
    "        # update network parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print loss values to the console during training\n",
    "        if iteration % 100 == 0:\n",
    "            print('Epoch: {}/{}'.format(e, 200),\n",
    "                  'Iteration: {}'.format(iteration),\n",
    "                  'Loss: {}'.format(loss_value))\n",
    "        \n",
    "        # print a little sample of text during training\n",
    "        if iteration % 1000 == 0:\n",
    "            predict(device, net, flags.initial_words, n_vocab, vocab_to_int, int_to_vocab, top_k=5)\n",
    "            torch.save(net.state_dict(), '../checkpoint_pt/model-{}-{}.pth'.format(iteration, loss_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModule(\n",
       "  (embedding): Embedding(71566, 64)\n",
       "  (lstm): LSTM(64, 64, batch_first=True)\n",
       "  (dense): Linear(in_features=64, out_features=71566, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNModule(n_vocab, flags.seq_size, flags.embedding_size, flags.lstm_size)\n",
    "model.load_state_dict(torch.load('../checkpoint_pt/model-1000.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________ The Krusty Krab! SpongeBob: Hey, you can have the greatest thing you go, Mr. Krabs. Squidward: You don't know. I have the little old little more thing is a few friend, I can't do it! I need the Krusty Krab! Mr. Krabs: And you were the little idea! Mr. Krabs the animals are we have a bunch and you have you two idea! Mr. Krabs? Squidward: And that's the Krusty Krab. I got you two one more idea! SpongeBob: Oh, that's not a great idea! SpongeBob: Hey, hey, hey, hey, that's the Krusty Krab. Squidward: You don't know. SpongeBob: And this isn't the little animals is not to do it. I got you were to get to my old new king and you can have a great of the king is you can do this. SpongeBob: And what I just have you were a bunch to be a few friend, I'm gonna be a little animals with your old time I can't get it! Squidward: And you can do it. Squidward: And I don't think I have you got a few friend, I can't go on to my idea! Squidward: Hey, hey, that's your old new Krabby Patty of a little more thing you go, Mr. Krabs. SpongeBob: I thought I just think you were gonna have the Krusty Krab with a few life is you can get me my own of a great idea, Patrick: You don't have the little old old idea! Squidward: Oh, that's it! Mr. Krabs, you have to go on the greatest Krusty Krab, the door is a few new Krusty Krab with the ground as he gets a hole with the animals with his face in front door] Mr. Krabs. Mr. Krabs is a little clown and the crowd and the urchin is in the ground in a Krabby Patty in the ground on a Krabby Patty on top of the ground as a bunch to SpongeBob as they both SpongeBob and the door and runs out from the door with a few head on top of a few jellyfish monster is covered on top with SpongeBob and SpongeBob and Patrick: And what do it is you to the Krusty Krab! I need a Krabby Patty on the door and runs up with a Krabby Patty. The door and runs into his mouth and runs back on his head in a few head to get his head] Oh, I can't have the time of your old Krabby Patty for a few more day and Barnacle Boy: You don't have to get your life to a little idea! Squidward: Hey, hey, the little idea! I don't need a good of my own old of this thing is your ice cream! Mr. Krabs, we don't have a little more thing is the door of his face with his house with the crowd in a hole with an ice cream. Squidward: And this time, I thought I have an gonna be the animals of your best friend, Mr. Krabs is gonna do it! Mr. Krabs in front and a Krabby Patties to a bunch to a bunch and SpongeBob walks off of SpongeBob on a bunch to SpongeBob on it] Squidward: I don't have a new time of my new time and Barnacle Boy's new animals of your own idea! I can't get it! Patrick: You don't have the Krusty Krab of a few friend, I can't have an idea. SpongeBob: And you think I have a new Krabby Patty for my best idea! I got it! SpongeBob: And this isn't the door with your ice cream] SpongeBob: Hey, what you think you're going out to the Krusty Krab and you can be my idea! Patrick: And I just got to get the animals to the animals and you're a little idea. Squidward: Oh, yeah, I'm just gonna have a good time to get me your life of the Krusty Krab, I'm gonna go out that is gonna go to my new new little old of my best idea! SpongeBob: Oh, I got it! Squidward: Hey, hey, hey, you can be the little little old little old new time for your best friend just have an time I can't go to the animals and you like your own time to do it on my friend, I just think of this thing you have you were gonna go on your new animals with this time I got it! Patrick: Hey, I have to do that. SpongeBob: You can be your best friend in front life is in the greatest urchins in front door on the Krabby Patty on top and runs off the crowd of the door to a hole into the crowd is still in a hole into his head] I can't do it. SpongeBob: I can't have a Krabby Patty! Patrick: Oh, no. SpongeBob: Oh, that's not a good thing I have the Krusty Krab! Squidward: You have a good idea! Squidward: You don't have the little old time and the little idea! Squidward: Oh, yeah. You got to do that. Mr. Krabs. Squidward: I got a good time to do you like a good idea! I can't go to a great friend, I just thought it are you? SpongeBob: Hey, hey, that's not not to do it. Squidward: You got it! Mr. Krabs. I have a Krabby Patty! Squidward: And this was my own of the door of the Krusty Krab. SpongeBob runs out to SpongeBob as SpongeBob in the Krusty Krab with a hole on his hand] Mr. Krabs: Hey, hey, you get your new best friend, I'm a little old good idea! I got it. SpongeBob: You got a little more idea! Patrick: Hey, that's it! I have to do it. SpongeBob: I can't be the door is a good thing and I don't want a little old idea! Mr. Krabs: Hey, what do I just thought you have to get a great of the door to the Krusty Krab! Squidward: I got it! Mr. Krabs: And that's not a bunch and you are a little good idea! SpongeBob: Oh, I have you have to go for you and I have the animals I have to get it! Mr. Krabs: I don't need to be the Krusty Krab] SpongeBob: You got a bunch on his face in the Krusty Krab] Squidward: And that's the greatest urchins is going into my best friend are you? Patrick: You can get a new little old time of a great idea! Squidward: Hey, you can be a great idea! I need you to be your life of a few\n"
     ]
    }
   ],
   "source": [
    "predict(device, model, flags.initial_words, n_vocab, vocab_to_int, int_to_vocab, top_k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
